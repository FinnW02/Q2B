[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "DS2 Finn Weber",
    "section": "",
    "text": "Finn Weber Matrikel Nummer: 00163853\nZiel dieser Projektarbeit ist es, mittels Textklassifikation Tweets zu analysieren. Ergebnis soll eine Klassifikation sein, bei der zwischen Hatespeech und keinem Hate Speech klassifiziert wird."
  },
  {
    "objectID": "posts/welcome/index.html#tokenisierung",
    "href": "posts/welcome/index.html#tokenisierung",
    "title": "DS2 Finn Weber",
    "section": "Tokenisierung",
    "text": "Tokenisierung\nJetzt beginne ich mit der Tokenisierung\n\nd_hate_token &lt;- d_hate_2 %&gt;%\n  unnest_tokens(word, tweet)\n\nprint(d_hate_token)\n\n# A tibble: 78,321 × 3\n      id class word        \n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1     0 other mayasolovely\n 2     0 other as          \n 3     0 other a           \n 4     0 other woman       \n 5     0 other you         \n 6     0 other shouldn't   \n 7     0 other complain    \n 8     0 other about       \n 9     0 other cleaning    \n10     0 other up          \n# ℹ 78,311 more rows\n\n\n\nlength(unique(d_hate_token$id))\n\n[1] 5593\n\n\n\nEntfernung Stopwords\nAls nächstes entferne ich Stopwords, da auch die keinen Mehrwert besitzen bei der Klassifikation\n\nlibrary(dplyr)\nlibrary(tidytext)\n\nstopwords_en &lt;- data.frame(word = stop_words$word)\n\n\nd_hate_clean &lt;- d_hate_token %&gt;%\n  anti_join(stopwords_en)\n\nJoining with `by = join_by(word)`\n\nprint(d_hate_clean)\n\n# A tibble: 40,418 × 3\n      id class word        \n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n 1     0 other mayasolovely\n 2     0 other woman       \n 3     0 other complain    \n 4     0 other cleaning    \n 5     0 other house       \n 6     0 other amp         \n 7     0 other trash       \n 8    40 other momma       \n 9    40 other pussy       \n10    40 other cats        \n# ℹ 40,408 more rows"
  },
  {
    "objectID": "posts/welcome/index.html#sentimentannalyse",
    "href": "posts/welcome/index.html#sentimentannalyse",
    "title": "DS2 Finn Weber",
    "section": "Sentimentannalyse",
    "text": "Sentimentannalyse\nJetzt beginne ich mit der Sentimentanalyse, um mir einen weiteren Überblick über die Daten zu verschaffen. Dafür benutze ich 3 Sentimentwörterbücher, die ich dann kombiniere.\n\nlibrary(tidytext)\n\nget_sentiments(\"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\n\nHier kombiniere ich die 3 Sentimentbücher unter combined_analysis.\n\nsent_afinn &lt;- get_sentiments(\"afinn\")\nsent_bing &lt;- get_sentiments(\"bing\")\nsent_nrc &lt;- get_sentiments(\"nrc\")\n\n\ncommon_words &lt;- Reduce(intersect, list(sent_afinn$word, sent_nrc$word, sent_bing$word))\n\ncombined_analysis &lt;- data.frame(word = common_words)\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_afinn, by = \"word\") %&gt;%\n  mutate(neg_pos_afinn = if_else(value &gt; 0, \"pos\", \"neg\"))\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_nrc, by = \"word\") %&gt;%\n  mutate(neg_pos_nrc = if_else(sentiment == \"positive\", \"pos\", \"neg\"))\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_bing, by = \"word\") %&gt;%\n  mutate(neg_pos_bing = if_else(value &gt; 0, \"pos\", \"neg\"))\n\nWarning in left_join(combined_analysis, sent_bing, by = \"word\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 730 of `x` matches multiple rows in `y`.\nℹ Row 20 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ncombined_analysis &lt;- combined_analysis[, -c(3:5)]\n\n\nnrow(combined_analysis)\n\n[1] 2154\n\nd_hatetoken_SA &lt;- d_hate_clean %&gt;%\ninner_join(combined_analysis)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., combined_analysis): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 136 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nnrow(d_hatetoken_SA)\n\n[1] 7004\n\nlength(unique(d_hatetoken_SA$id))\n\n[1] 1668\n\nd_hatetoken_SA\n\n# A tibble: 7,004 × 6\n      id class word     value sentiment.y neg_pos_bing\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       \n 1     0 other complain    -2 negative    neg         \n 2     0 other complain    -2 negative    neg         \n 3     0 other complain    -2 negative    neg         \n 4    70 other scream      -2 negative    neg         \n 5    70 other scream      -2 negative    neg         \n 6    70 other scream      -2 negative    neg         \n 7    70 other scream      -2 negative    neg         \n 8    70 other scream      -2 negative    neg         \n 9    75 other adorable     3 positive    pos         \n10    75 other adorable     3 positive    pos         \n# ℹ 6,994 more rows\n\n\nSentimentwerte geplottet\n\nd_hatetoken_SA %&gt;% \n  ggplot(aes(value)) + \n  geom_histogram() +\n  labs(x = \"Sentimentswert\",\n       y = \"Anzahl\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nd_hatetoken_SA %&gt;%\n  count(word, neg_pos_bing, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos_bing) %&gt;%\n  slice_max(n, n = 12)%&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word, fill = neg_pos_bing)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos_bing, scales = \"free_y\")"
  },
  {
    "objectID": "posts/welcome/index.html#recipe-erstellung",
    "href": "posts/welcome/index.html#recipe-erstellung",
    "title": "DS2 Finn Weber",
    "section": "Recipe Erstellung",
    "text": "Recipe Erstellung\nIm ersten Schritt erstelle ich mein Recept für die Modelle. Hierbei nutze ich step_stopwords, um Stopwörter zu entfernen, step_stem, um Wortstämme zu analysieren, step_tfidf zur tfidf implementierung und step_text_normalization zur weiteren Textbereinigung.\n\nRec1 &lt;- \n  recipe(class ~ ., data = Hate_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(senti = get_sentiment(tweet)) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;% \n  step_stem(tweet) %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_tfidf(tweet)"
  },
  {
    "objectID": "posts/welcome/index.html#preppen-und-backen",
    "href": "posts/welcome/index.html#preppen-und-backen",
    "title": "DS2 Finn Weber",
    "section": "preppen und backen",
    "text": "preppen und backen\n\nrec1_prepped &lt;- prep(Rec1)\n\nd_rec1 &lt;- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n\n# A tibble: 6 × 103\n     id class       senti tfidf_tweet_ tfidf_tweet_amp tfidf_tweet_ass\n  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1    85 hate speech -0.25            0               0               0\n2    90 hate speech  1               0               0               0\n3   111 hate speech -1               0               0               0\n4   206 hate speech -0.75            0               0               0\n5   263 hate speech -1.75            0               0               0\n6   320 hate speech -0.35            0               0               0\n# ℹ 97 more variables: tfidf_tweet_back &lt;dbl&gt;, tfidf_tweet_best &lt;dbl&gt;,\n#   tfidf_tweet_big &lt;dbl&gt;, tfidf_tweet_bird &lt;dbl&gt;, tfidf_tweet_bitch &lt;dbl&gt;,\n#   tfidf_tweet_black &lt;dbl&gt;, tfidf_tweet_boi &lt;dbl&gt;, tfidf_tweet_browni &lt;dbl&gt;,\n#   tfidf_tweet_call &lt;dbl&gt;, tfidf_tweet_can &lt;dbl&gt;, tfidf_tweet_charli &lt;dbl&gt;,\n#   tfidf_tweet_color &lt;dbl&gt;, tfidf_tweet_come &lt;dbl&gt;, tfidf_tweet_cracker &lt;dbl&gt;,\n#   tfidf_tweet_da &lt;dbl&gt;, tfidf_tweet_dai &lt;dbl&gt;, tfidf_tweet_eat &lt;dbl&gt;,\n#   tfidf_tweet_even &lt;dbl&gt;, tfidf_tweet_ever &lt;dbl&gt;, tfidf_tweet_everi &lt;dbl&gt;, …\n\n\nEntschieden habe ich mich für ein NaiveBayes und ein XgBoost Modell. Beide definiere ich hier. Beim XgBoost Modell tune ich Parameter im folgenden. Trees habe ich auf 1000 gesetzt, um overfitting zu verhindern.\n\nNaiveBayes &lt;- naive_Bayes() %&gt;% \n  set_mode(\"classification\") %&gt;%\n  set_engine(\"naivebayes\")\n\n\nXgBoost &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = 1000, \n  tree_depth = tune(), \n  min_n = tune(), \n  ) %&gt;%\n  set_engine(\"xgboost\", nthreads = parallel::detectCores()) %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/welcome/index.html#workflowset",
    "href": "posts/welcome/index.html#workflowset",
    "title": "DS2 Finn Weber",
    "section": "Workflowset",
    "text": "Workflowset\nErstellung des WFsets, für die mehreren Modelle.\n\npreproc &lt;- list(Rec1 = Rec1)\nmodels &lt;- list(NavB = NaiveBayes, XgB = XgBoost)\n \n \nall_workflows &lt;- workflow_set(preproc, models)\n\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(Hate_train,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 2 resampling: Rec1_NavB\n\n\n✔ 1 of 2 resampling: Rec1_NavB (13.1s)\n\n\ni 2 of 2 tuning:     Rec1_XgB\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 2 tuning:     Rec1_XgB (6m 3.6s)"
  },
  {
    "objectID": "posts/welcome/index.html#performanz",
    "href": "posts/welcome/index.html#performanz",
    "title": "DS2 Finn Weber",
    "section": "Performanz",
    "text": "Performanz\n\ncollect_metrics(model_set) %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  slice_max(mean, n = 3)\n\n# A tibble: 3 × 9\n  wflow_id .config          preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.935    10 0.00575\n2 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.919    10 0.00761\n3 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.909    10 0.00827\n\n\n\nautoplot(model_set)\n\n\n\n\n\n\n\n\nDie Modelle schneiden beiede eigentlich gut ab. Klar erkennbar ist aber, dass die XgBoost Modelle besser performen als die naive_bayes. Im nächsten Schritt suche ich daher das beste Modell raus.\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean) %&gt;% \n  head(10)\n\n# A tibble: 10 × 9\n   wflow_id  .config        preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.935    10 0.00575\n 2 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.919    10 0.00761\n 3 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.912    10 0.00434\n 4 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.909    10 0.00827\n 5 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.906    10 0.00417\n 6 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.895    10 0.00297\n 7 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.886    10 0.00892\n 8 Rec1_NavB Preprocessor1… recipe  naiv… roc_auc binary     0.878    10 0.00481\n 9 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.867    10 0.00588\n10 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.858    10 0.00866\n\nbest_model_params &lt;-\nextract_workflow_set_result(model_set, \"Rec1_XgB\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\nbest_model_params\n\n# A tibble: 1 × 4\n   mtry min_n tree_depth .config             \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1     4     4         12 Preprocessor1_Model1\n\n\nDas Rec1_XgB Modell schneidet hier am besten ab und wird daher im weiteren Verlauf von mir verwendet."
  },
  {
    "objectID": "posts/welcome/index.html#finalisieren",
    "href": "posts/welcome/index.html#finalisieren",
    "title": "DS2 Finn Weber",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nNr1_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"Rec1_XgB\")\n\nbest_wf_finalized &lt;- \n  Nr1_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = Hate_train)\n\n[21:13:01] WARNING: src/learner.cc:767: \nParameters: { \"nthreads\" } are not used.\n\n\nDie Ergebnisse schaue ich mir nochmal genauer an, um spätere Prognosen damit zu vergleichen."
  },
  {
    "objectID": "posts/welcome/index.html#roc-auc-kurve",
    "href": "posts/welcome/index.html#roc-auc-kurve",
    "title": "DS2 Finn Weber",
    "section": "ROC-Auc Kurve",
    "text": "ROC-Auc Kurve\n\nwf_preds &lt;-\n  collect_predictions(model_set)\n\nwf_preds %&gt;%\n  group_by(wflow_id) %&gt;% \n  roc_curve(truth = class, `.pred_hate speech`) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nDie Fläche unterhalb der Kurve ist durchaus beachtlich und unterstützt die These, dass das Modell gut abschneidet."
  },
  {
    "objectID": "posts/welcome/index.html#varibale-importance",
    "href": "posts/welcome/index.html#varibale-importance",
    "title": "DS2 Finn Weber",
    "section": "Varibale Importance",
    "text": "Varibale Importance\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\n\n\n\n\nDas Hinzufügen des tfdif steps im recipe scheint sich Eindeutig gelohnt zu haben."
  },
  {
    "objectID": "posts/welcome/index.html#predicten",
    "href": "posts/welcome/index.html#predicten",
    "title": "DS2 Finn Weber",
    "section": "Predicten",
    "text": "Predicten\nDas Modell nutze ich nun zum Predicten von Werten\n\nHT2 &lt;- Hate_test %&gt;% \n  select(-class)\n\n\nPredicion &lt;- predict(fit_final, HT2)\n\nPredicion &lt;- as.data.frame(Predicion)\n\n\nPredicion$ID &lt;- 1:nrow(Predicion)\nHate_test$ID &lt;- 1:nrow(Hate_test)\nmerged_df &lt;- merge(Predicion, Hate_test, by = \"ID\")\n\n\nHate_Test2 &lt;-\n  merged_df %&gt;%  \n  mutate(class = as.factor(class))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(Hate_Test2,\n           truth = class,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.912\n2 f_meas   binary         0.811\n\n\nDie Metrics hierbei sind wirklich sehr gut. Das XgBoost Modell schneidet hierbei sehr gut ab."
  },
  {
    "objectID": "posts/welcome/index.html#nutzung-des-venv",
    "href": "posts/welcome/index.html#nutzung-des-venv",
    "title": "DS2 Finn Weber",
    "section": "Nutzung des Venv",
    "text": "Nutzung des Venv\n\nlibrary(reticulate)\n\nuse_virtualenv(\"C:\\\\Users\\\\finnw\\\\Desktop\\\\Env\\\\myenv\")\n\nHier installiere ich die nötigen Python Libraries\n\n#pip install pandas\n#pip install tensorflow\n#pip install torch\n#pip install transformers\n\nHier nutze ich die pipeline, um das Modell zu importieren\n\nimport pandas as pd\nimport tensorflow as tf\n\nWARNING:tensorflow:From C:\\Users\\finnw\\Desktop\\Env\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\nfrom transformers import pipeline\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nBereitstellung der Tweets für python\n\nTweets &lt;- Hate_test$tweet\n\nHier lasse ich das Modell laufen\n\ntweets = r.Tweets\nresults = classifier(tweets)\n\nSpeicherung der Ergebnisse\nhierfür nehme ich die Python Prediction und füge sie mit dem Testdatensatz zusammen, um diese dann mit den originalen Werten zu vergleichen.\n\nlabels &lt;- lapply(py$results, function(element) {\n  if (element$label == \"hate\") {\n    return(\"hate speech\")\n  } else {\n    return(\"other\")\n  }\n})\n\nHT3 &lt;- Hate_test %&gt;% \n  select(tweet, id, class)\n\n\n\nHT3$pred &lt;- unlist(labels)\n\n\nHT3$pred &lt;- factor(HT3$pred, levels = c(\"hate speech\", \"other\"))\nHT3$class &lt;- factor(HT3$class, levels = c(\"hate speech\", \"other\"))\n\n\n# Faktorisierung\n\nHT3 &lt;- HT3 %&gt;% \n  mutate(class = as.factor(class)) %&gt;% \n  mutate(pred = as.factor(pred))"
  },
  {
    "objectID": "posts/welcome/index.html#metrics",
    "href": "posts/welcome/index.html#metrics",
    "title": "DS2 Finn Weber",
    "section": "Metrics",
    "text": "Metrics\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(HT3,\n           truth = class,\n           estimate = pred)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.874\n2 f_meas   binary         0.788\n\n\nZusammenfassend kann man also sagen, dass das XgBoost Modell auf diesen Daten immer noch besser performt, als das Roberta Modell. Beide performen jedoch sehr gut und haben ihre eigenen Vorteile. Das Roberta Modell läuft bei mir z.B. weitaus schneller durch. Auch das Naive Bayes Modell passt solide zu dem Datensatz, performt aber allgemein schlechter. Die Aufarbeitung der Daten im ersten Schritt (Entfernung der https etc) und die tfidf Implementierung haben hier bei der Analyse den größten Unterschied gemacht und für gute Ergebnisse gesorgt."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Q2B",
    "section": "",
    "text": "DS2 Finn Weber\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nFinn Weber\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html#sentimentanalyse",
    "href": "posts/welcome/index.html#sentimentanalyse",
    "title": "DS2 Finn Weber",
    "section": "Sentimentanalyse",
    "text": "Sentimentanalyse\nJetzt beginne ich mit der Sentimentanalyse, um mir einen weiteren Überblick über die Daten zu verschaffen. Dafür benutze ich 3 Sentimentwörterbücher, die ich dann kombiniere.\n\nlibrary(tidytext)\n\nget_sentiments(\"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\n\nHier kombiniere ich die 3 Sentimentbücher unter combined_analysis.\n\nsent_afinn &lt;- get_sentiments(\"afinn\")\nsent_bing &lt;- get_sentiments(\"bing\")\nsent_nrc &lt;- get_sentiments(\"nrc\")\n\n\ncommon_words &lt;- Reduce(intersect, list(sent_afinn$word, sent_nrc$word, sent_bing$word))\n\ncombined_analysis &lt;- data.frame(word = common_words)\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_afinn, by = \"word\") %&gt;%\n  mutate(neg_pos_afinn = if_else(value &gt; 0, \"pos\", \"neg\"))\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_nrc, by = \"word\") %&gt;%\n  mutate(neg_pos_nrc = if_else(sentiment == \"positive\", \"pos\", \"neg\"))\n\ncombined_analysis &lt;- left_join(combined_analysis, sent_bing, by = \"word\") %&gt;%\n  mutate(neg_pos_bing = if_else(value &gt; 0, \"pos\", \"neg\"))\n\nWarning in left_join(combined_analysis, sent_bing, by = \"word\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 730 of `x` matches multiple rows in `y`.\nℹ Row 20 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ncombined_analysis &lt;- combined_analysis[, -c(3:5)]\n\n\nnrow(combined_analysis)\n\n[1] 2154\n\nd_hatetoken_SA &lt;- d_hate_clean %&gt;%\ninner_join(combined_analysis)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., combined_analysis): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 136 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nnrow(d_hatetoken_SA)\n\n[1] 7004\n\nlength(unique(d_hatetoken_SA$id))\n\n[1] 1668\n\nd_hatetoken_SA\n\n# A tibble: 7,004 × 6\n      id class word     value sentiment.y neg_pos_bing\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       \n 1     0 other complain    -2 negative    neg         \n 2     0 other complain    -2 negative    neg         \n 3     0 other complain    -2 negative    neg         \n 4    70 other scream      -2 negative    neg         \n 5    70 other scream      -2 negative    neg         \n 6    70 other scream      -2 negative    neg         \n 7    70 other scream      -2 negative    neg         \n 8    70 other scream      -2 negative    neg         \n 9    75 other adorable     3 positive    pos         \n10    75 other adorable     3 positive    pos         \n# ℹ 6,994 more rows\n\n\nSentimentwerte geplottet\n\nd_hatetoken_SA %&gt;% \n  ggplot(aes(value)) + \n  geom_histogram() +\n  labs(x = \"Sentimentswert\",\n       y = \"Anzahl\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nd_hatetoken_SA %&gt;%\n  count(word, neg_pos_bing, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos_bing) %&gt;%\n  slice_max(n, n = 12)%&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word, fill = neg_pos_bing)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos_bing, scales = \"free_y\")"
  },
  {
    "objectID": "posts/welcome/index.html#performance",
    "href": "posts/welcome/index.html#performance",
    "title": "DS2 Finn Weber",
    "section": "Performance",
    "text": "Performance\n\ncollect_metrics(model_set) %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  slice_max(mean, n = 3)\n\n# A tibble: 3 × 9\n  wflow_id .config          preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.935    10 0.00575\n2 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.919    10 0.00761\n3 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.909    10 0.00827\n\n\n\nautoplot(model_set)\n\n\n\n\n\n\n\n\nDie Modelle schneiden beide eigentlich gut ab. Klar erkennbar ist aber, dass die XgBoost Modelle besser performen als die naive_bayes. Im nächsten Schritt suche ich daher das beste Modell raus.\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean) %&gt;% \n  head(10)\n\n# A tibble: 10 × 9\n   wflow_id  .config        preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.935    10 0.00575\n 2 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.919    10 0.00761\n 3 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.912    10 0.00434\n 4 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.909    10 0.00827\n 5 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.906    10 0.00417\n 6 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.895    10 0.00297\n 7 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.886    10 0.00892\n 8 Rec1_NavB Preprocessor1… recipe  naiv… roc_auc binary     0.878    10 0.00481\n 9 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.867    10 0.00588\n10 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.858    10 0.00866\n\nbest_model_params &lt;-\nextract_workflow_set_result(model_set, \"Rec1_XgB\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\nbest_model_params\n\n# A tibble: 1 × 4\n   mtry min_n tree_depth .config             \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;chr&gt;               \n1     4     4         12 Preprocessor1_Model1\n\n\nDas Rec1_XgB Modell schneidet hier am besten ab und wird daher im weiteren Verlauf von mir verwendet."
  },
  {
    "objectID": "posts/welcome/index.html#variable-importance",
    "href": "posts/welcome/index.html#variable-importance",
    "title": "DS2 Finn Weber",
    "section": "Variable Importance",
    "text": "Variable Importance\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\n\n\n\n\nDas Hinzufügen des tfdif steps im recipe scheint sich Eindeutig gelohnt zu haben."
  }
]