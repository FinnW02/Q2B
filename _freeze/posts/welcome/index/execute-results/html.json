{
  "hash": "6a8026372da98a655b4e6e3b9ffe0bd0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"DS2 Finn Weber\"\nauthor: \"Finn Weber\"\ndate: \"2024-02-06\"\n---\n\n\nFinn Weber\nMatrikel Nummer: 00163853\n\nZiel dieser Projektarbeit ist es, mittels Textklassifikation Tweets zu Analysieren. Ergebnis soll eine Klassifikation sein, bei der zwischen Hatespeech und keinem Hate Speech klassifiziert wird. \n\n\n\n# Datensatz und Pakete Laden \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nd_hate <- read_csv(\"C:/Users/finnw/Downloads/d_hate.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 5593 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): tweet, class\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nView(d_hate)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(tidymodels)\nlibrary(stringr)\nlibrary(httr)\nlibrary(textrecipes)\nlibrary(sentimentr)\nlibrary(syuzhet)\nlibrary(vip)\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(caret)\nlibrary(yardstick)\nlibrary(discrim)\n```\n:::\n\n\n\n# EDA \n\n\nIm ersten Schritt entferne ich Patterns aus den Tweets, die für die Analyse keinen Wert haben. Anhand der vorhandenen Links kann man m.M.n definitiv nicht vorhersagen, ob es sich um Hatespeech handelt. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_hate_2 <- d_hate %>%\n  mutate(tweet = str_remove_all(tweet, pattern = 'RT\\\\s*|http[s]?://\\\\S+|\\\\d+')) %>% \n  mutate(tweet = str_remove_all(tweet, pattern = '\\\\bhttp\\\\b|\\\\bt.co\\\\b')) \n```\n:::\n\n\nIm nächsten Schritt erstelle ich Train und Test Daten, für die spätere Modellierung.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ntrain_test_split <- initial_split(d_hate_2, prop = .7, strata = class)\nHate_train <- training(train_test_split)\nHate_test <- testing(train_test_split)\n```\n:::\n\n\n\n## Tokenisierung\n\nJetzt beginne ich mit der Tokenisierung \n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_hate_token <- d_hate_2 %>%\n  unnest_tokens(word, tweet)\n\nprint(d_hate_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 78,321 × 3\n      id class word        \n   <dbl> <chr> <chr>       \n 1     0 other mayasolovely\n 2     0 other as          \n 3     0 other a           \n 4     0 other woman       \n 5     0 other you         \n 6     0 other shouldn't   \n 7     0 other complain    \n 8     0 other about       \n 9     0 other cleaning    \n10     0 other up          \n# ℹ 78,311 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(d_hate_token$id))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5593\n```\n\n\n:::\n:::\n\n\n\n### Entfernung Stopwords \n\nAls nächstes Entferne ich Stopwords, da auch die keinen Mehrwert besitzen bei der Klassifikation \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidytext)\n\nstopwords_en <- data.frame(word = stop_words$word)\n\n\nd_hate_clean <- d_hate_token %>%\n  anti_join(stopwords_en)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(d_hate_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 40,418 × 3\n      id class word        \n   <dbl> <chr> <chr>       \n 1     0 other mayasolovely\n 2     0 other woman       \n 3     0 other complain    \n 4     0 other cleaning    \n 5     0 other house       \n 6     0 other amp         \n 7     0 other trash       \n 8    40 other momma       \n 9    40 other pussy       \n10    40 other cats        \n# ℹ 40,408 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Sentimentannalyse \n\nJetzt beginne ich mit der Sentimentanalyse, um mir einen weiteren Überblick über die Daten zu verschaffen. Dafür benutze ich 3 Sentimentwörterbücher, die ich dann kombiniere.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n\nget_sentiments(\"afinn\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,477 × 2\n   word       value\n   <chr>      <dbl>\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nget_sentiments(\"bing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nget_sentiments(\"nrc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13,872 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n```\n\n\n:::\n:::\n\n\nHier kombiniere ich die 3 Sentimentbücher unter combined_analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsent_afinn <- get_sentiments(\"afinn\")\nsent_bing <- get_sentiments(\"bing\")\nsent_nrc <- get_sentiments(\"nrc\")\n\n\ncommon_words <- Reduce(intersect, list(sent_afinn$word, sent_nrc$word, sent_bing$word))\n\ncombined_analysis <- data.frame(word = common_words)\n\ncombined_analysis <- left_join(combined_analysis, sent_afinn, by = \"word\") %>%\n  mutate(neg_pos_afinn = if_else(value > 0, \"pos\", \"neg\"))\n\ncombined_analysis <- left_join(combined_analysis, sent_nrc, by = \"word\") %>%\n  mutate(neg_pos_nrc = if_else(sentiment == \"positive\", \"pos\", \"neg\"))\n\ncombined_analysis <- left_join(combined_analysis, sent_bing, by = \"word\") %>%\n  mutate(neg_pos_bing = if_else(value > 0, \"pos\", \"neg\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in left_join(combined_analysis, sent_bing, by = \"word\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 730 of `x` matches multiple rows in `y`.\nℹ Row 20 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n```\n\n\n:::\n\n```{.r .cell-code}\ncombined_analysis <- combined_analysis[, -c(3:5)]\n\n\nnrow(combined_analysis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2154\n```\n\n\n:::\n\n```{.r .cell-code}\nd_hatetoken_SA <- d_hate_clean %>%\ninner_join(combined_analysis)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in inner_join(., combined_analysis): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 136 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(d_hatetoken_SA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7004\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(d_hatetoken_SA$id))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1668\n```\n\n\n:::\n\n```{.r .cell-code}\nd_hatetoken_SA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7,004 × 6\n      id class word     value sentiment.y neg_pos_bing\n   <dbl> <chr> <chr>    <dbl> <chr>       <chr>       \n 1     0 other complain    -2 negative    neg         \n 2     0 other complain    -2 negative    neg         \n 3     0 other complain    -2 negative    neg         \n 4    70 other scream      -2 negative    neg         \n 5    70 other scream      -2 negative    neg         \n 6    70 other scream      -2 negative    neg         \n 7    70 other scream      -2 negative    neg         \n 8    70 other scream      -2 negative    neg         \n 9    75 other adorable     3 positive    pos         \n10    75 other adorable     3 positive    pos         \n# ℹ 6,994 more rows\n```\n\n\n:::\n:::\n\n\nSentimentwerte geplottet  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_hatetoken_SA %>% \n  ggplot(aes(value)) + \n  geom_histogram() +\n  labs(x = \"Sentimentswert\",\n       y = \"Anzahl\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_hatetoken_SA %>%\n  count(word, neg_pos_bing, sort = TRUE) %>%\n  ungroup() %>%\n  group_by(neg_pos_bing) %>%\n  slice_max(n, n = 12)%>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = neg_pos_bing)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos_bing, scales = \"free_y\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n# Modellierung - Tidymodels\n\n\n## Recipe Erstellung\n\nIm ersten Schritt erstelle ich mein Recept für die Modelle. Hierbei nutze ich  step_stopwords, um Stopwörter zu entfernen, step_stem, um Wortstämme zu analysieren, step_tfidf zur tfidf implementierung und   step_text_normalization zur weiteren Textbereinigung.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRec1 <- \n  recipe(class ~ ., data = Hate_train) %>%\n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(tweet) %>%\n  step_mutate(senti = get_sentiment(tweet)) %>% \n  step_tokenize(tweet, token = \"words\") %>%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %>% \n  step_stem(tweet) %>%\n  step_tokenfilter(tweet, max_tokens = 1e2) %>%\n  step_tfidf(tweet) \n```\n:::\n\n\n## preppen und backen\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1_prepped <- prep(Rec1)\n\nd_rec1 <- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 103\n     id class       senti tfidf_tweet_ tfidf_tweet_amp tfidf_tweet_ass\n  <dbl> <fct>       <dbl>        <dbl>           <dbl>           <dbl>\n1    85 hate speech -0.25            0               0               0\n2    90 hate speech  1               0               0               0\n3   111 hate speech -1               0               0               0\n4   206 hate speech -0.75            0               0               0\n5   263 hate speech -1.75            0               0               0\n6   320 hate speech -0.35            0               0               0\n# ℹ 97 more variables: tfidf_tweet_back <dbl>, tfidf_tweet_best <dbl>,\n#   tfidf_tweet_big <dbl>, tfidf_tweet_bird <dbl>, tfidf_tweet_bitch <dbl>,\n#   tfidf_tweet_black <dbl>, tfidf_tweet_boi <dbl>, tfidf_tweet_browni <dbl>,\n#   tfidf_tweet_call <dbl>, tfidf_tweet_can <dbl>, tfidf_tweet_charli <dbl>,\n#   tfidf_tweet_color <dbl>, tfidf_tweet_come <dbl>, tfidf_tweet_cracker <dbl>,\n#   tfidf_tweet_da <dbl>, tfidf_tweet_dai <dbl>, tfidf_tweet_eat <dbl>,\n#   tfidf_tweet_even <dbl>, tfidf_tweet_ever <dbl>, tfidf_tweet_everi <dbl>, …\n```\n\n\n:::\n:::\n\n\nEntschieden habe ich mich für ein NaiveBayes und ein XgBoost Modell. Beide definiere ich hier. Beim XgBoost Modell tune ich Parameter im folgenden. Trees habe ich auf 1000 gesetzt, um overfitting zu verhindern.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nNaiveBayes <- naive_Bayes() %>% \n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\n\nXgBoost <- \n  boost_tree(\n  mtry = tune(), \n  trees = 1000, \n  tree_depth = tune(), \n  min_n = tune(), \n  ) %>%\n  set_engine(\"xgboost\", nthreads = parallel::detectCores()) %>%\n  set_mode(\"classification\") \n```\n:::\n\n\n## Workflowset\n\nErstellung des WFsets, für die mehreren Modelle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- list(Rec1 = Rec1)\nmodels <- list(NavB = NaiveBayes, XgB = XgBoost)\n \n \nall_workflows <- workflow_set(preproc, models)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_set <-\nall_workflows %>%\nworkflow_map(\n  resamples = vfold_cv(Hate_train,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni\tNo tuning parameters. `fit_resamples()` will be attempted\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni 1 of 2 resampling: Rec1_NavB\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ 1 of 2 resampling: Rec1_NavB (12.6s)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni 2 of 2 tuning:     Rec1_XgB\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ 2 of 2 tuning:     Rec1_XgB (5m 51.3s)\n```\n\n\n:::\n:::\n\n\n## Performanz \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(model_set) %>% \n  filter(.metric == \"roc_auc\") %>% \n  slice_max(mean, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n  wflow_id .config          preproc model .metric .estimator  mean     n std_err\n  <chr>    <chr>            <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n1 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.935    10 0.00575\n2 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.919    10 0.00761\n3 Rec1_XgB Preprocessor1_M… recipe  boos… roc_auc binary     0.909    10 0.00827\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(model_set)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nDie Modelle schneiden beiede eigentlich gut ab. Klar erkennbar ist aber, dass die XgBoost Modelle besser performen als die naive_bayes. Im nächsten Schritt suche ich daher das beste Modell raus. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_set %>% \n  collect_metrics() %>% \n  arrange(-mean) %>% \n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 9\n   wflow_id  .config        preproc model .metric .estimator  mean     n std_err\n   <chr>     <chr>          <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n 1 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.935    10 0.00575\n 2 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.919    10 0.00761\n 3 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.912    10 0.00434\n 4 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.909    10 0.00827\n 5 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.906    10 0.00417\n 6 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.895    10 0.00297\n 7 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.886    10 0.00892\n 8 Rec1_NavB Preprocessor1… recipe  naiv… roc_auc binary     0.878    10 0.00481\n 9 Rec1_XgB  Preprocessor1… recipe  boos… accura… binary     0.867    10 0.00588\n10 Rec1_XgB  Preprocessor1… recipe  boos… roc_auc binary     0.858    10 0.00866\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_model_params <-\nextract_workflow_set_result(model_set, \"Rec1_XgB\") %>% \n  select_best()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_model_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n   mtry min_n tree_depth .config             \n  <int> <int>      <int> <chr>               \n1     4     4         12 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nDas Rec1_XgB Modell schneidet hier am besten ab und wird daher im weiteren Verlauf von mir verwendet.\n\n\n## Finalisieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNr1_wf <- \nall_workflows %>% \n  extract_workflow(\"Rec1_XgB\")\n\nbest_wf_finalized <- \n  Nr1_wf %>% \n  finalize_workflow(best_model_params)\n\nfit_final <- fit(best_wf_finalized, data = Hate_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[20:17:27] WARNING: src/learner.cc:767: \nParameters: { \"nthreads\" } are not used.\n```\n\n\n:::\n:::\n\n\n\nDie Ergebnisse schaue ich mir nochmal genauer an, um spätere Prognosen damit zu vergleichen.\n\n\n\n## ROC-Auc Kurve\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_preds <-\n  collect_predictions(model_set)\n\nwf_preds %>%\n  group_by(wflow_id) %>% \n  roc_curve(truth = class, `.pred_hate speech`) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nDie Fläche unterhalb der Kurve ist durchaus beachtlich und unterstützt die These, dass das Modell gut abschneidet.\n\n## Varibale Importance \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final %>% \n  extract_fit_parsnip() %>% \n  vip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nDas Hinzufügen des tfdif steps im recipe scheint sich Eindeutig gelohnt zu haben.\n\n\n## Predicten\n\nDas Modell nutze ich nun zum Predicten von Werten\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHT2 <- Hate_test %>% \n  select(-class)\n\n\nPredicion <- predict(fit_final, HT2)\n\nPredicion <- as.data.frame(Predicion)\n\n\nPredicion$ID <- 1:nrow(Predicion)\nHate_test$ID <- 1:nrow(Hate_test)\nmerged_df <- merge(Predicion, Hate_test, by = \"ID\")\n\n\nHate_Test2 <-\n  merged_df %>%  \n  mutate(class = as.factor(class))\n\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Hate_Test2,\n           truth = class,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.912\n2 f_meas   binary         0.811\n```\n\n\n:::\n:::\n\n\nDie Metrics hierbei sind wirklich sehr gut. Das XgBoost Modell schneidet hierbei sehr gut ab. \n\n# Klassifikation mit dem Facebook Roberta Modell\n\nZur Benutzung des Facebook Roberta Modells benötige ich eine Python venv, welche ich außerhalb des R Projekts in Python erstellt habe. Hierbei benutze ich den Befehl pipeline aus der transformers Library, um das Modell zu importieren und zum Predicten zu nutzen.\n\n\n## Nutzung des Venv \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\nuse_virtualenv(\"C:\\\\Users\\\\finnw\\\\Desktop\\\\Env\\\\myenv\")\n```\n:::\n\n\nHier installiere ich die nötigen Python Libraries \n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#pip install pandas\n#pip install tensorflow\n#pip install torch\n#pip install transformers\n\n```\n:::\n\n\nHier nutze ich die pipeline, um das Modell zu importieren\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport tensorflow as tf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWARNING:tensorflow:From C:\\Users\\finnw\\Desktop\\Env\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n```\n\n\n:::\n\n```{.python .cell-code}\n\n\n\nfrom transformers import pipeline\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\n```\n:::\n\n\nBereitstellung der Tweets für python\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTweets <- Hate_test$tweet\n```\n:::\n\n\nHier lasse ich das Modell laufen\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntweets = r.Tweets\nresults = classifier(tweets)\n```\n:::\n\n\nSpeicherung der Ergebnisse \n\nhierfür nehme ich die Python Prediction und füge sie mit dem test Datensatz zusammen, um diese dann mit den originalen Werten zu vergleichen.\n\n::: {.cell}\n\n```{.r .cell-code}\nlabels <- lapply(py$results, function(element) {\n  if (element$label == \"hate\") {\n    return(\"hate speech\")\n  } else {\n    return(\"other\")\n  }\n})\n\nHT3 <- Hate_test %>% \n  select(tweet, id, class)\n\n\n\nHT3$pred <- unlist(labels)\n\n\nHT3$pred <- factor(HT3$pred, levels = c(\"hate speech\", \"other\"))\nHT3$class <- factor(HT3$class, levels = c(\"hate speech\", \"other\"))\n\n\n# Faktorisierung\n\nHT3 <- HT3 %>% \n  mutate(class = as.factor(class)) %>% \n  mutate(pred = as.factor(pred))\n```\n:::\n\n## Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics2 <- metric_set(accuracy, f_meas)\nmy_metrics2(HT3,\n           truth = class,\n           estimate = pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.874\n2 f_meas   binary         0.788\n```\n\n\n:::\n:::\n\n\n\nZusammenfassend kann man also sagen, dass das XgBoost Modell auf diesen Daten immer noch besser performt, als das Roberta Modell. Beide performen jedoch sehr gut und haben ihre eigenen Vorteile. Das Roberta Modell läuft bei mir z.B. weitaus schneller durch. Auch das Naive Bayes Modell passt solide zu dem Datensatz, performt aber allgemein schlechter. Die Aufarbeitung der Daten im ersten Schritt (entfernung der https etc) und die tfidf Implementierung haben hier bei der Analyse den größten Unterschied gemacht und für gute Ergebnisse gesorgt.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}